{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from dataset_utils import split_data, TimeSeriesDataset, min_max_normalization\n",
    "from model.lstm import LSTMMdel\n",
    "from model.nlinear import NLinear\n",
    "from model.SegRNN import SegRNN\n",
    "from model.PatchTST import PatchTST,Config\n",
    "from train_utils import Trainer\n",
    "from evaluate_utils import Evaluator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "import threading\n",
    "import json\n",
    "from json_utils import NpEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../../dataset\"\n",
    "index_field = \"timestamp\"\n",
    "data_field = \"num_request\"\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "N_HISTORY = 32\n",
    "N_LOOKBACK = 4\n",
    "N_PREDICT = 2\n",
    "LR = 1e-2\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "RESULT_ROOT_PATH=\"results\"\n",
    "MODEL_NAME=\"lstm\"\n",
    "# MODEL_NAME=\"nlinear\"\n",
    "# MODEL_NAME=\"segrnn\"\n",
    "# MODEL_NAME=\"patchtst\"\n",
    "\n",
    "TRAIN_EPOCH=20\n",
    "\n",
    "# Trainer also supports training by early stop\n",
    "EARLY_STOP_GAIN=0.01\n",
    "EARLY_STOP_LOSS=0.02\n",
    "\n",
    "start_index = -1\n",
    "start_index_end = -1\n",
    "start_index_lock = threading.Lock()\n",
    "\n",
    "N_THREAD = 8\n",
    "\n",
    "train_gt_scaled = {}\n",
    "train_pd_scaled = {}\n",
    "test_gt_scaled = {}\n",
    "test_pd_scaled = {}\n",
    "\n",
    "train_gt_original = {}\n",
    "train_pd_original = {}\n",
    "test_gt_original = {}\n",
    "test_pd_original = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_file_list(dataset_path: str) -> List[str]:\n",
    "    return os.listdir(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(csv_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df[index_field].to_numpy(), df[data_field].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_on_history(history_data: np.ndarray) -> Tuple[Evaluator, MinMaxScaler, Evaluator, MinMaxScaler]:\n",
    "    train_set, test_set = split_data(history_data, N_LOOKBACK, N_PREDICT)\n",
    "    train_set, train_scaler = min_max_normalization(train_set)\n",
    "    test_set, test_scaler = min_max_normalization(test_set)\n",
    "    train_dataset = TimeSeriesDataset(train_set, N_LOOKBACK, N_PREDICT)\n",
    "    test_dataset = TimeSeriesDataset(test_set, N_LOOKBACK, N_PREDICT)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    if MODEL_NAME==\"lstm\":\n",
    "        model = LSTMMdel(1, 64, N_PREDICT, 2).to(DEVICE)\n",
    "    elif MODEL_NAME==\"nlinear\":\n",
    "        model=NLinear(N_LOOKBACK,N_PREDICT).to(DEVICE)\n",
    "    elif MODEL_NAME==\"segrnn\":\n",
    "        model = SegRNN(seq_len=N_LOOKBACK, pred_len=N_PREDICT, enc_in=1, d_model=64, dropout=0.5, rnn_type=\"lstm\", dec_way=\"rmf\", seg_len=1, channel_id=False, revin=False).to(DEVICE)\n",
    "    elif MODEL_NAME==\"patchtst\":\n",
    "        model = PatchTST(configs=Config(enc_in=1, seq_len=N_LOOKBACK, pred_len=N_PREDICT, e_layers=1, n_heads=4, d_model=16, d_ff=16, dropout=0.5, fc_dropout=0.5, head_dropout=0.5, individual=False, patch_len=1, stride=1, padding_patch=False, revin=False, affine=False, subtract_last=False, decomposition=False, kernel_size=1)).to(DEVICE)\n",
    "    \n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if 'weight' in name:\n",
    "    #         init.xavier_normal_(param)\n",
    "    #     elif 'bias' in name:\n",
    "    #         init.constant_(param, 0.0)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    trainer = Trainer(model, train_dataloader, loss_fn, optimizer, num_epochs=TRAIN_EPOCH, early_stop_gain=EARLY_STOP_GAIN, early_stop_loss=EARLY_STOP_LOSS, lr_scheduler=None, device=DEVICE)\n",
    "    # use `trainer.train_by_early_stop()` as an alternative\n",
    "    trainer.train_by_epoch()\n",
    "    train_evaluator = Evaluator(model, train_dataloader, loss_fn, DEVICE)\n",
    "    train_evaluator.evaluate()\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_evaluator = Evaluator(model, test_dataloader, loss_fn, DEVICE)\n",
    "    test_evaluator.evaluate()\n",
    "    return train_evaluator, train_scaler, test_evaluator, test_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_dict_scaled(train_evaluator: Evaluator, test_evaluator: Evaluator, local_start_index: int):\n",
    "    train_gt_scaled[local_start_index] = train_evaluator.get_gt()\n",
    "    train_pd_scaled[local_start_index] = train_evaluator.get_pd()\n",
    "    test_gt_scaled[local_start_index+N_HISTORY-N_LOOKBACK] = test_evaluator.get_gt()\n",
    "    test_pd_scaled[local_start_index+N_HISTORY-N_LOOKBACK] = test_evaluator.get_pd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_dict_original(train_evaluator: Evaluator, train_scaler: MinMaxScaler, test_evaluator: Evaluator, test_scaler: MinMaxScaler, local_start_index: int):\n",
    "    tmp_train_gt_scaled, tmp_train_pd_scaled, tmp_test_gt_scaled, tmp_test_pd_scaled = train_evaluator.get_gt(), train_evaluator.get_pd(), test_evaluator.get_gt(), test_evaluator.get_pd()\n",
    "\n",
    "    tmp_train_gt_original = np.hstack([train_scaler.inverse_transform(tmp_train_gt_scaled[:, i_dim]) for i_dim in range(tmp_train_gt_scaled.shape[1])])\n",
    "    tmp_train_gt_original = np.expand_dims(tmp_train_gt_original, -1)\n",
    "    tmp_train_pd_original = np.hstack([train_scaler.inverse_transform(tmp_train_pd_scaled[:, i_dim]) for i_dim in range(tmp_train_pd_scaled.shape[1])])\n",
    "    tmp_train_pd_original = np.expand_dims(tmp_train_pd_original, -1)\n",
    "    tmp_test_gt_original = np.hstack([test_scaler.inverse_transform(tmp_test_gt_scaled[:, i_dim]) for i_dim in range(tmp_test_gt_scaled.shape[1])])\n",
    "    tmp_test_gt_original = np.expand_dims(tmp_test_gt_original, -1)\n",
    "    tmp_test_pd_original = np.hstack([test_scaler.inverse_transform(tmp_test_pd_scaled[:, i_dim]) for i_dim in range(tmp_test_pd_scaled.shape[1])])\n",
    "    tmp_test_pd_original = np.expand_dims(tmp_test_pd_original, -1)\n",
    "\n",
    "    train_gt_original[local_start_index] = tmp_train_gt_original\n",
    "    train_pd_original[local_start_index] = tmp_train_pd_original\n",
    "    test_gt_original[local_start_index+N_HISTORY-N_LOOKBACK] = tmp_test_gt_original\n",
    "    test_pd_original[local_start_index+N_HISTORY-N_LOOKBACK] = tmp_test_pd_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_learning(np_data: np.ndarray):\n",
    "    global start_index, start_index_end\n",
    "    while start_index < start_index_end:\n",
    "        start_index_lock.acquire()\n",
    "        if start_index > start_index_end:\n",
    "            start_index_lock.release()\n",
    "        else:\n",
    "            start_index += 1\n",
    "            local_start_index = start_index\n",
    "            start_index_lock.release()\n",
    "            print(str(threading.current_thread().name) + \"\\t\" + str(local_start_index))\n",
    "            history_data = np_data[local_start_index:local_start_index+N_HISTORY]\n",
    "            train_evaluator, train_scaler, test_evaluator, test_scaler = learn_on_history(history_data)\n",
    "            save_to_dict_scaled(train_evaluator, test_evaluator, local_start_index)\n",
    "            save_to_dict_original(train_evaluator, train_scaler, test_evaluator, test_scaler, local_start_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(result_dict: Dict[int, np.ndarray], file_name: str):\n",
    "    save_dir = os.path.join(RESULT_ROOT_PATH, MODEL_NAME)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    result_list=[result_dict[key] for key in sorted(result_dict.keys())]\n",
    "    np_result=np.array(result_list)\n",
    "    with open(os.path.join(save_dir, file_name), \"w\") as f:\n",
    "        json.dump(np_result, f, indent=4, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** The final result outputed to file is different from that in `offline_training`. The `online_training` result has one more dimension in result data. In this notebook, the result looks like `N_SEG x N_SAMPLES x N_PREDICT x1` where `N_SEG` is the number of segments, calculated as `ceil(LEN_DATA/N_HISTORY)`, `N_SAMPLES` is the number of training/test samples. This is caused by the sliding-window way of training. In the `offline_training` notebook, the result looks like `N_SAMPLES x N_PREDICT x 1` where `N_SAMPLES` is the sample number of each train/val/test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_list = get_data_file_list(dataset_path)\n",
    "for file_name in data_file_list:\n",
    "    \n",
    "    np_index, np_data = read_dataset(os.path.join(dataset_path, file_name))\n",
    "    np_data = np_data.reshape((-1, 1))\n",
    "    \n",
    "    start_index = -1\n",
    "    start_index_end = len(np_data)-N_HISTORY\n",
    "    \n",
    "    train_gt_scaled = {}\n",
    "    train_pd_scaled = {}\n",
    "    test_gt_scaled = {}\n",
    "    test_pd_scaled = {}\n",
    "\n",
    "    train_gt_original = {}\n",
    "    train_pd_original = {}\n",
    "    test_gt_original = {}\n",
    "    test_pd_original = {}\n",
    "    \n",
    "    thread_list = []\n",
    "    for _ in range(N_THREAD):\n",
    "        # use multi-threading training to improve the efficiency.\n",
    "        thread_list.append(threading.Thread(target=run_learning, args=[np_data]))\n",
    "    for t in thread_list:\n",
    "        t.start()\n",
    "    for t in thread_list:\n",
    "        t.join()\n",
    "\n",
    "    save_to_file(train_gt_scaled, file_name.split(\".\")[0]+\"_gt_train_scaled.json\")\n",
    "    save_to_file(train_pd_scaled, file_name.split(\".\")[0]+\"_pd_train_scaled.json\")\n",
    "    save_to_file(test_gt_scaled, file_name.split(\".\")[0]+\"_gt_test_scaled.json\")\n",
    "    save_to_file(test_pd_scaled, file_name.split(\".\")[0]+\"_pd_test_scaled.json\")\n",
    "    \n",
    "    save_to_file(train_gt_original, file_name.split(\".\")[0]+\"_gt_train_original.json\")\n",
    "    save_to_file(train_pd_original, file_name.split(\".\")[0]+\"_pd_train_original.json\")\n",
    "    save_to_file(test_gt_original, file_name.split(\".\")[0]+\"_gt_test_original.json\")\n",
    "    save_to_file(test_pd_original, file_name.split(\".\")[0]+\"_pd_test_original.json\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoscaling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
