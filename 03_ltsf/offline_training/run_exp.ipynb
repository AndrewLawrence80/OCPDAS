{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dataset_utils import split_data, TimeSeriesDataset, min_max_normalization\n",
    "from model.lstm import LSTMMdel\n",
    "from model.nlinear import NLinear\n",
    "from model.SegRNN import SegRNN\n",
    "from model.PatchTST import PatchTST,Config\n",
    "from train_utils import Trainer\n",
    "from evaluate_utils import Evaluator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from json_utils import NpEncoder\n",
    "from typing import List, Tuple\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../../dataset\"\n",
    "INDEX_FIELD = \"timestamp\"\n",
    "DATA_FIELD = \"num_request\"\n",
    "RESULT_ROOT_PATH=\"results\"\n",
    "MODEL_NAME=\"lstm\"\n",
    "# MODEL_NAME=\"nlinear\"\n",
    "# MODEL_NAME=\"segrnn\"\n",
    "# MODEL_NAME=\"patchtst\"\n",
    "\n",
    "N_LOOKBACK = 4\n",
    "N_PREDICT = 2\n",
    "N_EPOCHS = 40\n",
    "BATCH_SIZE=16\n",
    "LR = 1e-3\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SCHEDULER_MILESTONE = [30, 35]\n",
    "SCHEDULER_GAMMA = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_file_list(dataset_path: str) -> List[str]:\n",
    "    return os.listdir(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(csv_path: str,index_field:str,data_field:str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df[index_field].to_numpy(), df[data_field].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_on_csv(csv_path: str) -> Tuple[Evaluator, MinMaxScaler, Evaluator, MinMaxScaler, Evaluator, MinMaxScaler]:\n",
    "    np_index, np_data = read_dataset(csv_path, INDEX_FIELD, DATA_FIELD)\n",
    "    np_data = np_data.reshape((-1, 1))\n",
    "    train_set, val_set, test_set = split_data(np_data, 0.6, 0.2, 0.2)\n",
    "    train_set, train_scaler = min_max_normalization(train_set)\n",
    "    val_set, val_scaler = min_max_normalization(val_set)\n",
    "    test_set, test_scaler = min_max_normalization(test_set)\n",
    "    train_dataset = TimeSeriesDataset(train_set, N_LOOKBACK, N_PREDICT)\n",
    "    val_dataset = TimeSeriesDataset(val_set, N_LOOKBACK, N_PREDICT)\n",
    "    test_dataset = TimeSeriesDataset(test_set, N_LOOKBACK, N_PREDICT)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    if MODEL_NAME == \"lstm\":\n",
    "        model = LSTMMdel(1, 64, N_PREDICT, 1).to(DEVICE)\n",
    "    elif MODEL_NAME == \"nlinear\":\n",
    "        model = NLinear(N_LOOKBACK, N_PREDICT).to(DEVICE)\n",
    "    elif MODEL_NAME == \"segrnn\":\n",
    "        model = SegRNN(seq_len=N_LOOKBACK, pred_len=N_PREDICT, enc_in=1, d_model=64, dropout=0.5, rnn_type=\"lstm\", dec_way=\"rmf\", seg_len=1, channel_id=False, revin=False).to(DEVICE)\n",
    "    elif MODEL_NAME == \"patchtst\":\n",
    "        model = PatchTST(configs=Config(enc_in=1, seq_len=N_LOOKBACK, pred_len=N_PREDICT, e_layers=1, n_heads=4, d_model=16, d_ff=16, dropout=0.5, fc_dropout=0.5, head_dropout=0.5, individual=False, patch_len=1, stride=1, padding_patch=False, revin=False, affine=False, subtract_last=False, decomposition=False, kernel_size=1)).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, SCHEDULER_MILESTONE, SCHEDULER_GAMMA)\n",
    "    lr_scheduler = None\n",
    "    loss_fn = nn.MSELoss()\n",
    "    trainer = Trainer(model, train_dataloader, loss_fn, optimizer, N_EPOCHS, lr_scheduler, DEVICE)\n",
    "    trainer.train()\n",
    "    train_evaluator = Evaluator(model, train_dataloader, loss_fn, DEVICE)\n",
    "    train_evaluator.evaluate()\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    val_evaluator = Evaluator(model, val_dataloader, loss_fn, DEVICE)\n",
    "    test_evaluator = Evaluator(model, test_dataloader, loss_fn, DEVICE)\n",
    "    val_evaluator.evaluate()\n",
    "    test_evaluator.evaluate()\n",
    "    return train_evaluator, train_scaler, val_evaluator, val_scaler, test_evaluator, test_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(x: np.ndarray, file_name: str):\n",
    "    save_dir = os.path.join(RESULT_ROOT_PATH, MODEL_NAME)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    with open(os.path.join(save_dir, file_name), \"w\") as f:\n",
    "        json.dump(x, f, indent=4, cls=NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scaled_results(train_evaluator: Evaluator, val_evaluator: Evaluator,  test_evaluator: Evaluator, file_name: str):\n",
    "    train_gt_scaled, train_pd_scaled, val_gt_scaled, val_pd_scaled, test_gt_scaled, test_pd_scaled = train_evaluator.get_gt(), train_evaluator.get_pd(), val_evaluator.get_gt(), val_evaluator.get_pd(), test_evaluator.get_gt(), test_evaluator.get_pd()\n",
    "\n",
    "    save_results(train_gt_scaled, file_name.split(\".\")[0]+\"_gt_train_scaled.json\")\n",
    "    save_results(train_pd_scaled, file_name.split(\".\")[0]+\"_pd_train_scaled.json\")\n",
    "    save_results(val_gt_scaled, file_name.split(\".\")[0]+\"_gt_val_scaled.json\")\n",
    "    save_results(val_pd_scaled, file_name.split(\".\")[0]+\"_pd_val_scaled.json\")\n",
    "    save_results(test_gt_scaled, file_name.split(\".\")[0]+\"_gt_test_scaled.json\")\n",
    "    save_results(test_pd_scaled, file_name.split(\".\")[0]+\"_pd_test_scaled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_original_results(train_evaluator: Evaluator, train_scaler: MinMaxScaler, val_evaluator: Evaluator, val_scaler: MinMaxScaler, test_evaluator: Evaluator, test_scaler: MinMaxScaler, file_name: str):\n",
    "    train_gt_scaled, train_pd_scaled, val_gt_scaled, val_pd_scaled, test_gt_scaled, test_pd_scaled = train_evaluator.get_gt(), train_evaluator.get_pd(), val_evaluator.get_gt(), val_evaluator.get_pd(), test_evaluator.get_gt(), test_evaluator.get_pd()\n",
    "    \n",
    "    train_gt_original=np.hstack([train_scaler.inverse_transform(train_gt_scaled[:,i_dim]) for i_dim in range(train_gt_scaled.shape[1])])\n",
    "    train_gt_original=np.expand_dims(train_gt_original,-1)\n",
    "    train_pd_original=np.hstack([train_scaler.inverse_transform(train_pd_scaled[:,i_dim]) for i_dim in range(train_pd_scaled.shape[1])])\n",
    "    train_pd_original=np.expand_dims(train_pd_original,-1)\n",
    "    val_gt_original=np.hstack([val_scaler.inverse_transform(val_gt_scaled[:,i_dim]) for i_dim in range(val_gt_scaled.shape[1])])\n",
    "    val_gt_original=np.expand_dims(val_gt_original,-1)\n",
    "    val_pd_original=np.hstack([val_scaler.inverse_transform(val_pd_scaled[:,i_dim]) for i_dim in range(val_pd_scaled.shape[1])])\n",
    "    val_pd_original=np.expand_dims(val_pd_original,-1)\n",
    "    test_gt_original=np.hstack([test_scaler.inverse_transform(test_gt_scaled[:,i_dim]) for i_dim in range(test_gt_scaled.shape[1])])\n",
    "    test_gt_original=np.expand_dims(test_gt_original,-1)\n",
    "    test_pd_original=np.hstack([test_scaler.inverse_transform(test_pd_scaled[:,i_dim]) for i_dim in range(test_pd_scaled.shape[1])])\n",
    "    test_pd_original=np.expand_dims(test_pd_original,-1)\n",
    "\n",
    "    save_results(train_gt_original, file_name.split(\".\")[0]+\"_gt_train_original.json\")\n",
    "    save_results(train_pd_original, file_name.split(\".\")[0]+\"_pd_train_original.json\")\n",
    "    save_results(val_gt_original, file_name.split(\".\")[0]+\"_gt_val_original.json\")\n",
    "    save_results(val_pd_original, file_name.split(\".\")[0]+\"_pd_val_original.json\")\n",
    "    save_results(test_gt_original, file_name.split(\".\")[0]+\"_gt_test_original.json\")\n",
    "    save_results(test_pd_original, file_name.split(\".\")[0]+\"_pd_test_original.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_list = get_data_file_list(DATASET_PATH)\n",
    "for file_name in data_file_list:\n",
    "    print(\"learning on %s\" % (file_name))\n",
    "    train_evaluator, train_scaler, val_evaluator, val_scaler, test_evaluator, test_scaler = learn_on_csv(os.path.join(DATASET_PATH, file_name))\n",
    "    save_scaled_results(train_evaluator, val_evaluator, test_evaluator, file_name)\n",
    "    save_original_results(train_evaluator, train_scaler, val_evaluator, val_scaler, test_evaluator, test_scaler, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoscaling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
